\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\title{11-711 Assignment 2 : Retrieval Augmented Generation System}

\author{Rajeev Veeraraghavan \thanks{This work was done as part of the 11-711 course at Carnegie Mellon University.
The assignment started off with a group of 3 members, but after 21st February, I had to complete the assignment individually.
Permission to work alone was taken from the course instructor. Permission to use material that the group had worked on together, prior to splitting up, was also taken from the course instructor. Shared work has been attributed to contributors.} \\
\\
  \texttt{rveerara@andrew.cmu.edu}
}

\begin{document}

\maketitle
\begin{abstract}
This report summarizes the work done to build a stand alone Retrieval augmented generation system to answers questions on CMU (Carnegie Mellon University).
The system uses publicly available data to build a knowledge base, and LLama2-7b model to generate answers to questions.
\end{abstract}

\section{Introduction}

\section{Data Creation}

\subsection{Raw Data Collection}
\subsubsection[short]{Which Data was Collected ?}
All collected raw data were from the publicly available sources mentioned on the assignment's github repository \cite{assignment-github}.
All the documents listed on the page were used because they broadly covered the topics that would be asked in the question set.
\subsubsection{Data Extraction}
\bfseries{LTI Faculty} \mdseries{The LTI faculty page was scraped using BeautifulSoup to get the names of the faculty members and their research interests. 
The scraped text was stored in raw fashion with one file per faculty member.
These files were transformed into a single JSON file and also into a descriptive text file where each sentenced contained exactly one faculty member's information.}
\\
\bfseries{LTI Faculty Research} 
\mdseries{ Semantic Scholar was used to get the research papers (and abstract, metadata) of the faculty members.
The data was parsed into a JSON file which had a list where each member contained information about one paper.}
\\
\bfseries{Academics @ LTI}
\mdseries{The old LTI website landing page was scraped using BeautifulSoup to get gists of the courses offered by LTI.
The scraped text was stored in raw fashion with one file per course, and fed directly to the embedder. The program handbooks
were downloaded as pdfs and were then parsed using PyPdf to generate text files. The PDFs were directly used in one set of embeddings while the text files were used in another set of other embeddings.
}
\\
\bfseries{CMU Courses}
\mdseries{The CMU course schedule catalog was scraped using BeautifulSoup to get the schedules of all courses. The scraped data was converted into a JSON file and into a combined text file where each sentence contained a worded form of one course's information.
The academic calendars for 2022-23 and 2023-24 were downloaded as PDFs and parsed into text files. Both formats were fed into different sets of embedders.}
\\
\bfseries{History, Athletics and Events}
\mdseries{The relevant pages were scraped with BeautifulSoup, and PDFs were downloaded where available. As the documents here had different inherent structures, most of the data was manually copied into text files and parsed with separate scripts to convert the raw data into sentence form.}

\subsection{Annotations}
\subsubsection{Type and Quantity of Annotations}
A total of 115 test annotation pairs were created manually - 31 for the history and events category, 15 for the LTI program handbook category, 30 for the CMU courses category, 20 for the LTI faculty category, and 19 for the LTI faculty research category.
This split covered the different types of data collected, and the different types of questions that could be asked on them. No train annotations were created, since there was no fine tuning or few shot prompting being performed. The annotations were used to select the best models and embeddings.
All the reference answers were kept down to 5 words with multiple variations being accepted (eg - 11711; 11-711; 11711 course; 11-711 course; 11711 course at CMU; 11-711 course at CMU etc). Reference answers were kept as short as possible with the goal of selecting a model that gave concise answers (to maintain the F1 score), while maintaining recall.
This likely resulted in low f1 scores, but the idea behind this is to select a model that is concise.
As I worked alone on this assignment, I could not calculate an inter annotator agreement score.

\section{Model Details}

\subsection{Constraints}
The RAG system was built on a GCP compute instance - n1-highmem-4 with 4 core CPU, 1 NVIDIA T4 GPU, 26 GB RAM, and 160 GB SSD. This influenced the choice of inference and embedding models.
Langchain \cite{lang-chain} was used to build the embedder and retriever modules due to its ease of use and integrations available.
\subsection{Embedding Models and Vector Store}
I chose to use the bge-large-en-v1.5 BGE embedding model as it is an established model in the top 15 of the MTEB leader board. Further, the v1.5 model update embedding model: release bge-en-v1.5 embedding model alleviates the issue of the similarity distribution and has ehanced retrieval ability without instruction, compared to earlier bge models.
\\
I experimented with the chunk sizes for both embeddings to find out the which size fits best.
I used recursive chunking for greater granularity and variety of text, varying the chunk overlap between 0.1 and 0.4 times the size of the chunk size to see what effect the overlap had on the retrieval results.  The embeddings were stored in a Chroma vector database due to its support for multiple formats and ease of use for a first time user.
\\
I also evaluated llama2-7b embeddings and compared the two sets of embeddings to see which one gave better results. My hypothesis was that the llama2-7b embeddings along with the llama2-7b model for inference would provide comparable results to using the BGE embedding with llama2-7b and similar sized models.
\subsection{Inference Models}
I used the following suite of open source models available with Ollama: llama2-7b, mistral, open-chat and neural-chat. I expected the 4 models to give different performances across the metrics measured while all being light weight enough to run locally with a reasonable infernce time.
Among the 4 models, I expected to choose 2 for the final unseen test set submission, based on their relative peformance F1, recall and exact match. I did not expect any of these models to do very well at exact matching as they are all chat models.

\section{Results}

\subsection{Embedding Models}

I compared performance across 2 embeddings to choose an embedding model - \textbf{bge-large-en-v1.5 and llama2-7b}. I used F1 score as the primary metric to compare the embeddings, along with recall, precision, rogue score and exact match. The results are shown in tables \ref{tab:accents} and \ref{tab:accents2}.
\begin{table}
\centering
  \begin{tabular}{ l c c r }
  \hline
    \textbf{Model} & \textbf{Chunk Size} & \textbf{F1 Score \%} & \textbf{Recall}\\
    \hline
    \verb|Llama2 | & 500 &  {5.6} & {} \\
    \verb|Openchat | & 500 & {7.8}  & {}\\ 
    \verb|Neural Chat | & 500 &{7.8}  & {}\\ 
    \verb|Llama2 | & 1000 &  {5.6}  & {}\\
    \verb|Openchat | & 1000 & {7.8} \ & {} \\ 
    \verb|Neural Chat | & 1000 &{7.8}  & {}\\ 
    \hline
  \end{tabular}
\caption{Evaluation results with Llama2 embeddings}
\label{tab:accents}
\end{table}

\begin{table}
  \centering
    \begin{tabular}{ l c c r }
    \hline
      \textbf{Model} & \textbf{Chunk Size} & \textbf{F1 Score \%} & \textbf{Recall}\\
      \hline
      \verb|Llama2 | & 500 &  {5.6} & {} \\
      \verb|Openchat | & 500 & {7.8}  & {}\\ 
      \verb|Neural Chat | & 500 &{7.8}  & {}\\ 
      \verb|Llama2 | & 1000 &  {5.6}  & {}\\
      \verb|Openchat | & 1000 & {7.8} \ & {} \\ 
      \verb|Neural Chat | & 1000 &{7.8}  & {}\\ 
      \hline
    \end{tabular}
  \caption{Evaluation results with BGE embeddings}
  \label{tab:accents2}
  \end{table}

  Which embedding was better across evaluation metrics, chunk sizes and models ?
  Was one embeddings signifactly better than the other ?

\subsection{Comparison of Models}

After selecting the embedding, I compared the performance of the 4 models across the same metrics as the embeddings. The results are shown in tables \ref{tab:tab3}.
  
\begin{table}
  \centering
    \begin{tabular}{ l c c r }
    \hline
      \textbf{Model} & \textbf{Chunk Size} & \textbf{Retriever} & \textbf{F1 Score \%} \\
      \hline
      \verb|Llama2 | & 500 &  {} & {} \\
      \verb|Openchat | & 500 & {}  & {}\\ 
      \verb|Neural Chat | & 500 &{}  & {}\\ 
      \verb|Llama2 | & 1000 &  {}  & {}\\
      \verb|Openchat | & 1000 & {} \ & {} \\ 
      \verb|Neural Chat | & 1000 &{}  & {}\\ 
      \hline
    \end{tabular}
  \caption{Evaluation results with Llama2 embeddings}
  \label{tab:tab3}
  \end{table}

How do the models comapre across evaluation metrics, chunk sizes and models ?
Was one model signifactly better than the rest ?

\section{Analysis}
The annotations contained questions that required different types and numbers of documents to answer them. For example, most questions from the history category could be found from the context within one document. Similarly, questions from the schedules category that simply asked where or when a class was held were expected to be easier for the RAG system.
Questions from the LTI faculty and LTI research categories were also of two types - ones whose answers could be found within one document or ones that needed collection of information from multiple documents. By document, I refer to the chunked documents here.
Questions such as \textit{Which author of the paper titled " Unsupervised Dense Retrieval Training with Web Anchors" did not publish any other paper in 2023 ?} required information from all documents corresponding to LTI papers. This question was not expected to be answere correctly by any of the baseline models.
I categorized the question based on a hypothetical difficulty level, in terms of how many documents would be needed to capture the answer, and evaluated the peformance of the models I tsted across these difficulty levels. The results of this evaluation are given below.
\subsection{Single Document Questions}
\textbf{When was the Carnegie Plan initiated ?}
\bfseries{}
\textbf{}
\bfseries{}
\textbf{}
\bfseries{}

\subsection{Simple Multi Document Questions}

\subsection {Complex Multi Document Questions}
\section{Conclusion}


\clearpage
\section*{Acknowledgements}
I acklowledge the contributions of the following people to this assignment and report:
\\
\textbf{Sudeep Agarwal (sudeepag@andreew.cmu.edu)} \textit{contributed to the scraping of raw data for academic calendar schedule, LTI faculty and LTI faculty research}
\\
\textbf{Hugo C (hcontant@andrw.cmu.edu)} \textit{contributed to the scraping of raw data for Kiltie band, Scottie and Tartans}

\bibliography{custom}

\end{document}
