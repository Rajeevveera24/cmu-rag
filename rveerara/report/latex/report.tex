\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\title{11-711 Assignment 2 : Retrieval Augmented Generation System}

\author{Rajeev Veeraraghavan \thanks{This work was done as part of the 11-711 course at Carnegie Mellon University.
The assignment started off with a group of 3 members, but after 21st February, I had to complete the assignment individually.
Permission to work alone was taken from the course instructor. Permission to use material that the group had worked on together, prior to splitting up, was also taken from the course instructor. Shared work has been attributed to contributors.} \\
\\
  \texttt{rveerara@andrew.cmu.edu}
}

\begin{document}

\maketitle
\begin{abstract}
This report summarizes the work done to build a stand alone Retrieval augmented generation system to answers questions on CMU (Carnegie Mellon University).
The system uses publicly available data to build a knowledge base, and LLama2-7b model to generate answers to questions.
\end{abstract}

\section{Introduction}

\section{Data Creation}

\subsection{Raw Data Collection}
\subsubsection[short]{Which Data was Collected ?}
All collected raw data were from the publicly available sources mentioned on the assignment's github repository \cite{assignment-github}.
All the documents listed on the page were used because they broadly covered the topics that would be asked in the question set.
\subsubsection{Data Extraction}
\bfseries{LTI Faculty} \mdseries{The LTI faculty page was scraped using BeautifulSoup to get the names of the faculty members and their research interests. 
The scraped text was stored in raw fashion with one file per faculty member.
These files were transformed into a single JSON file and also into a descriptive text file where each sentenced contained exactly one faculty member's information.}
\\
\bfseries{LTI Faculty Research} 
\mdseries{ Semantic Scholar was used to get the research papers (and abstract, metadata) of the faculty members.
The data was parsed into a JSON file which had a list where each member contained information about one paper.}
\\
\bfseries{Academics @ LTI}
\mdseries{The old LTI website landing page was scraped using BeautifulSoup to get gists of the courses offered by LTI.
The scraped text was stored in raw fashion with one file per course, and fed directly to the embedder. The program handbooks
were downloaded as pdfs and were then parsed using PyPdf to generate text files. The PDFs were directly used in one set of embeddings while the text files were used in another set of other embeddings.
}
\\
\bfseries{CMU Courses}
\mdseries{The CMU course schedule catalog was scraped using BeautifulSoup to get the schedules of all courses. The scraped data was converted into a JSON file and into a combined text file where each sentence contained a worded form of one course's information.
The academic calendars for 2022-23 and 2023-24 were downloaded as PDFs and parsed into text files. Both formats were fed into different sets of embedders.}
\\
\bfseries{History, Athletics and Events}
\mdseries{The relevant pages were scraped with BeautifulSoup, and PDFs were downloaded where available. As the documents here had different inherent structures, most of the data was manually copied into text files and parsed with separate scripts to convert the raw data into sentence form.}

\subsection{Annotations}
\subsubsection{Type and Quantity of Annotations}
A total of 115 test annotation pairs were created manually - 31 for the history and events category, 15 for the LTI program handbook category, 30 for the CMU courses category, 20 for the LTI faculty category, and 19 for the LTI faculty research category.
This split covered the different types of data collected, and the different types of questions that could be asked on them. No train annotations were created, since there was no fine tuning or few shot prompting being performed. The annotations were used to select the best models and embeddings.
All the reference answers were kept down to 5 words with multiple variations being accepted (eg - 11711; 11-711; 11711 course; 11-711 course; 11711 course at CMU; 11-711 course at CMU etc). Reference answers were kept as short as possible with the goal of selecting a model that gave concise answers (to maintain the F1 score), while maintaining recall.
This likely resulted in low f1 scores, but the idea behind this is to select a model that is concise.
As I worked alone on this assignment, I could not calculate an inter annotator agreement score.

\section{Model Details}

\subsection{Constraints}
The RAG system was built on a GCP compute instance - n1-highmem-4 with 4 core CPU, 1 NVIDIA T4 GPU, 26 GB RAM, and 160 GB SSD. This influenced the choice of inference and embedding models.
Langchain \cite{lang-chain} was used to build the embedder and retriever modules due to its ease of use and integrations available.
\subsection{Embedding Models and Vector Store}
I chose to use the bge-large-en-v1.5 BGE embedding model as it is an established model in the top 15 of the MTEB leader board. Further, the v1.5 model update embedding model: release bge-*-v1.5 embedding model alleviates the issue of the similarity distribution and has ehanced retrieval ability without instruction, compared to earlier bge models.
\\
I experimented with the chunk sizes for both embeddings to find out the which size fits best.
I used recursive chunking for greater granularity and variety of text, varying the chunk overlap between 0.1 and 0.4 times the size of the chunk size to see what effect the overlap had on the retrieval results.  The embeddings were stored in a Chroma vector database due to its support for multiple formats and ease of use for a first time user.
\\
I also evaluated llama2-7b embeddings and compared the two sets of embeddings to see which one gave better results. My hypothesis was that the llama2-7b embeddings along with the llama2-7b model for inference would provide comparable results to using the BGE embedding with llama2-7b and similar sized models.
\subsection{Inference Models}
I used the following suite of open source models available with Ollama: llama2-7b, mistral, open-chat and neural-chat. I expected the 4 models to give different performances across the metrics measured while all being light weight enough to run locally with a reasonable infernce time.
Among the 4 models, I expected to choose 2 for the final unseen test set submission, based on their relative peformance F1, recall and exact match. I did not expect any of these models to do very well at exact matching as they are all chat models.

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

\section{Results}

As shown in the table 1, the BGE embeddings gave better results than the llama2-7b embeddings across different inference models. The BGE embeddings had a higher F1 score, with a significant difference.
Further, the BGE embeddings with the Openchat model gave the best results, with a 21.1\% F1 score. 
\subsection{Comparison of Models}
\begin{table}
\centering
\begin{tabular}{lc}
\hline
\textbf{Embedding + Model} & \textbf{F1 Score \%}\\
\hline
\verb|Llama2 Embedding + Llama2| &  {5.6} \\
\verb|Llama2 Embedding + Mistral| & {5.9} \\
\verb|Llama2 Embedding + Open chat| & {7.8} \\ 
\verb|BGE + Lllama2| & {11.6} \\ 
\verb|BGE + Mistral| & {10.5} \\
\verb|BGE + Openchat| & {21.1}  \\ 
\hline
\end{tabular}
\caption{}
\label{tab:accents}
\end{table}



% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
% \centering
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\"a}| & {\"a} \\
% \verb|{\^e}| & {\^e} \\
% \verb|{\`i}| & {\`i} \\ 
% \verb|{\.I}| & {\.I} \\ 
% \verb|{\o}| & {\o} \\
% \verb|{\'u}| & {\'u}  \\ 
% \verb|{\aa}| & {\aa}  \\\hline
% \end{tabular}
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\c c}| & {\c c} \\ 
% \verb|{\u g}| & {\u g} \\ 
% \verb|{\l}| & {\l} \\ 
% \verb|{\~n}| & {\~n} \\ 
% \verb|{\H o}| & {\H o} \\ 
% \verb|{\v r}| & {\v r} \\ 
% \verb|{\ss}| & {\ss} \\
% \hline
% \end{tabular}
% \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
% \label{tab:accents}
% \end{table}

\section{Analysis}

\clearpage
\section*{Acknowledgements}

\textbf{Sudeep Agarwal sudeepag@andreew.cmu.edu} \textit{contributed to the scraping of raw data for academic calendar schedule, LTI faculty and LTI faculty research}
\\
\textbf{Hugo C hcontant@andrw.cmu.edu} \textit{contributed to the scraping of raw data for Kiltie band, Scottie and Tartans}

\bibliography{custom}

\end{document}
