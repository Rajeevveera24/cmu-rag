{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "# embedding_name = 'llama2'\n",
    "# persist_directory = DATABASE_PATH + embedding_name\n",
    "# embedding = OllamaEmbeddings(model=embedding_name)\n",
    "# vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "EMBEDDING_OPTIONS = ['tinyllama', 'llama2', 'gemma', 'mistral', 'neural-chat', 'openchat']\n",
    "VECTOR_STORE_DIRECTORIES = [DATABASE_PATH + embedding_name for embedding_name in EMBEDDING_OPTIONS]\n",
    "\n",
    "QUESTION_CATEGORIES = ['history']\n",
    "ANNOTATION_DIR = '/home/raj/nlp/cmu-rag/annotation/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(dir, embedding_name):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name))\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name), force=True)\n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store_non_ollama_embedding(dir, embedding_model):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model)\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model, force=True)\n",
    "    return vector_store\n",
    "\n",
    "def create_chain(vector_store, llm_model = 'llama2'):\n",
    "    rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "    prompt_message = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use as few words as possible and keep the answer concise. Do not mention the context in your response.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "    rag_prompt_llama.messages[0].prompt.template = prompt_message\n",
    "\n",
    "    llm = Ollama(model = llm_model)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_llama\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(category='', dir = ANNOTATION_DIR):\n",
    "    questions = []\n",
    "    for file in os.listdir(dir + category):\n",
    "        if file.endswith('questions.txt'):\n",
    "            with open(dir + category + '/' + file, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    questions.append(line.strip())\n",
    "    return questions\n",
    "\n",
    "def generate_answers(qa_chain, questions):\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions to answer\")\n",
    "    if not qa_chain:\n",
    "        raise ValueError(\"No qa_chain to answer questions\")\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        if not question:\n",
    "            continue\n",
    "        answer = dict()\n",
    "        answer_raw = qa_chain.invoke(question)\n",
    "        answer[\"raw\"] = answer_raw\n",
    "        num_lines = answer_raw.count('\\n')\n",
    "        answer[\"num_lines\"] = num_lines\n",
    "        lines = answer_raw.split('\\n')\n",
    "        if num_lines == 0:\n",
    "            answer[\"processed\"] = lines[0]\n",
    "        else:\n",
    "            answer_lines = []\n",
    "            for line in lines:\n",
    "                if \"i don't know\" not in line.lower():\n",
    "                    answer_lines.append(line)\n",
    "            answer[\"processed\"] = \" \".join(answer_lines)\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def write_answers(answers, file_name, append = False):\n",
    "    with open(file_name, 'a' if append else 'w') as f:\n",
    "        for answer in answers:\n",
    "            f.write(answer[\"processed\"] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering questions for category: history with embedding llama2\n",
      "Answers written to /home/raj/nlp/cmu-rag/annotation/test/history/llama2_answers.txt\n",
      "Answering questions for category: history with embedding llama2\n",
      "Answers written to /home/raj/nlp/cmu-rag/annotation/test/history/llama2_answers.txt\n"
     ]
    }
   ],
   "source": [
    "vector_stores, question_categories = ['llama2', 'bge-large-en'], ['history']\n",
    "# vector_stores, question_categories = VECTOR_STORE_DIRECTORIES, QUESTION_CATEGORIES\n",
    "embedding_models = [OllamaEmbeddings(model='llama2'), \n",
    "                    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True})\n",
    "                    ]\n",
    "\n",
    "\n",
    "for dir in vector_stores:\n",
    "    embedding_name = 'llama2'\n",
    "    # vector_store = load_vector_store(dir, embedding_name)\n",
    "    vector_store = load_vector_store_non_ollama_embedding(dir, embedding_models[vector_stores.index(dir)])\n",
    "    chain = create_chain(vector_store, embedding_name)\n",
    "    for category in question_categories:\n",
    "        print(\"Answering questions for category: {} with embedding {}\".format(category, embedding_name))\n",
    "        questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "        answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "        write_to_file = embedding_name + '_answers.txt'\n",
    "        write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n",
    "        print(f\"Answers written to {ANNOTATION_DIR + category + '/' + write_to_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m load_vector_store_non_ollama_embedding(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mDATABASE_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbge-large-en\u001b[39m\u001b[38;5;124m'\u001b[39m, embedding_model\u001b[38;5;241m=\u001b[39mhf)\n\u001b[1;32m     18\u001b[0m chain \u001b[38;5;241m=\u001b[39m create_chain(vector_store)\n\u001b[0;32m---> 19\u001b[0m questions \u001b[38;5;241m=\u001b[39m get_questions(\u001b[43mcategory\u001b[49m, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mANNOTATION_DIR)\n\u001b[1;32m     20\u001b[0m answers \u001b[38;5;241m=\u001b[39m generate_answers(qa_chain\u001b[38;5;241m=\u001b[39mchain, questions\u001b[38;5;241m=\u001b[39mquestions)\n\u001b[1;32m     21\u001b[0m write_to_file \u001b[38;5;241m=\u001b[39m embedding_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_answers.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'category' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#clear cache on cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedding_name = 'bge-large-en'\n",
    "vector_store = load_vector_store_non_ollama_embedding(dir=DATABASE_PATH+'bge-large-en', embedding_model=hf)\n",
    "chain = create_chain(vector_store)\n",
    "questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "write_to_file = embedding_name + '_answers.txt'\n",
    "write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file for questions:  0  to  10  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  10  to  20  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  20  to  30  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  30  to  40  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  40  to  50  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  50  to  60  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  60  to  70  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  70  to  80  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  80  to  90  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  90  to  100  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  100  to  110  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  110  to  120  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  120  to  130  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  130  to  140  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  140  to  150  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  150  to  160  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  160  to  170  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  170  to  180  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  180  to  190  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  190  to  200  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  200  to  210  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  210  to  220  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  220  to  230  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  230  to  240  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  240  to  250  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  250  to  260  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  260  to  270  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  270  to  280  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  280  to  290  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  290  to  300  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  300  to  310  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  310  to  320  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  320  to  330  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  330  to  340  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  340  to  350  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  350  to  360  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  360  to  370  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  370  to  380  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  380  to  390  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  390  to  400  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  400  to  410  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  410  to  420  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  420  to  430  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  430  to  440  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  440  to  450  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  450  to  460  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  460  to  470  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  470  to  480  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  480  to  490  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  490  to  500  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  500  to  510  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  510  to  520  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  520  to  530  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  530  to  540  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  540  to  550  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  550  to  560  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  560  to  570  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  570  to  580  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  580  to  590  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  590  to  600  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  600  to  610  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  610  to  620  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  620  to  630  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  630  to  640  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  640  to  650  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  650  to  660  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  660  to  670  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  670  to  680  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  680  to  690  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  690  to  700  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  700  to  710  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  710  to  720  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  720  to  730  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  730  to  740  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  740  to  750  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  750  to  760  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  760  to  770  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#clear cache on cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "TEST_QUESTION_PATH='/home/raj/nlp/cmu-rag/rveerara/system_outputs/'\n",
    "embedding_name = 'bge-large-en'\n",
    "vector_store = load_vector_store_non_ollama_embedding(dir=DATABASE_PATH+'bge-large-en', embedding_model=hf)\n",
    "chain = create_chain(vector_store)\n",
    "questions = get_questions(dir=TEST_QUESTION_PATH)\n",
    "print(questions)\n",
    "\n",
    "for i in range(0, len(questions), 10):\n",
    "    question_set = questions[i:min(i+10, len(questions))]\n",
    "    answers = generate_answers(qa_chain=chain, questions=question_set)\n",
    "    write_to_file = TEST_QUESTION_PATH + 'system_output_1.txt'\n",
    "    print(\"writing to file for questions: \", i, \" to \", i+10, \" to file: \", write_to_file)\n",
    "    write_answers(answers, write_to_file, append=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
