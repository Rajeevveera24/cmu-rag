{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "# embedding_name = 'llama2'\n",
    "# persist_directory = DATABASE_PATH + embedding_name\n",
    "# embedding = OllamaEmbeddings(model=embedding_name)\n",
    "# vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "EMBEDDING_OPTIONS = ['tinyllama', 'llama2', 'gemma', 'mistral', 'neural-chat', 'openchat']\n",
    "VECTOR_STORE_DIRECTORIES = [DATABASE_PATH + embedding_name for embedding_name in EMBEDDING_OPTIONS]\n",
    "\n",
    "QUESTION_CATEGORIES = ['history']\n",
    "ANNOTATION_DIR = '/home/raj/nlp/cmu-rag/annotation/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(dir, embedding_name):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name))\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name), force=True)\n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store_non_ollama_embedding(dir, embedding_model):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model)\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model, force=True)\n",
    "    return vector_store\n",
    "\n",
    "def create_chain(vector_store, llm_model = 'llama2'):\n",
    "    rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "    prompt_message = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use as few words as possible and keep the answer concise. Do not mention the context in your response.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "    rag_prompt_llama.messages[0].prompt.template = prompt_message\n",
    "\n",
    "    llm = Ollama(model = llm_model)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_llama\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(category='', dir = ANNOTATION_DIR):\n",
    "    questions = []\n",
    "    for file in os.listdir(dir + category):\n",
    "        if file.endswith('questions.txt'):\n",
    "            with open(dir + category + '/' + file, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    questions.append(line.strip())\n",
    "    return questions\n",
    "\n",
    "def generate_answers(qa_chain, questions):\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions to answer\")\n",
    "    if not qa_chain:\n",
    "        raise ValueError(\"No qa_chain to answer questions\")\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        if not question:\n",
    "            continue\n",
    "        answer = dict()\n",
    "        answer_raw = qa_chain.invoke(question)\n",
    "        answer[\"raw\"] = answer_raw\n",
    "        num_lines = answer_raw.count('\\n')\n",
    "        answer[\"num_lines\"] = num_lines\n",
    "        lines = answer_raw.split('\\n')\n",
    "        if num_lines == 0:\n",
    "            answer[\"processed\"] = lines[0]\n",
    "        else:\n",
    "            answer_lines = []\n",
    "            for line in lines:\n",
    "                if \"i don't know\" not in line.lower():\n",
    "                    answer_lines.append(line)\n",
    "            answer[\"processed\"] = \" \".join(answer_lines)\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def write_answers(answers, file_name, append = False):\n",
    "    with open(file_name, 'a' if append else 'w') as f:\n",
    "        for answer in answers:\n",
    "            f.write(answer[\"processed\"] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering questions for category: history with embedding llama2\n",
      "Answers written to /home/raj/nlp/cmu-rag/annotation/test/history/llama2_answers.txt\n",
      "Answering questions for category: history with embedding llama2\n",
      "Answers written to /home/raj/nlp/cmu-rag/annotation/test/history/llama2_answers.txt\n"
     ]
    }
   ],
   "source": [
    "vector_stores, question_categories = ['llama2', 'bge-large-en'], ['history']\n",
    "# vector_stores, question_categories = VECTOR_STORE_DIRECTORIES, QUESTION_CATEGORIES\n",
    "embedding_models = [OllamaEmbeddings(model='llama2'), \n",
    "                    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\", model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True})\n",
    "                    ]\n",
    "\n",
    "\n",
    "for dir in vector_stores:\n",
    "    embedding_name = 'llama2'\n",
    "    # vector_store = load_vector_store(dir, embedding_name)\n",
    "    vector_store = load_vector_store_non_ollama_embedding(dir, embedding_models[vector_stores.index(dir)])\n",
    "    chain = create_chain(vector_store, embedding_name)\n",
    "    for category in question_categories:\n",
    "        print(\"Answering questions for category: {} with embedding {}\".format(category, embedding_name))\n",
    "        questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "        answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "        write_to_file = embedding_name + '_answers.txt'\n",
    "        write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n",
    "        print(f\"Answers written to {ANNOTATION_DIR + category + '/' + write_to_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m load_vector_store_non_ollama_embedding(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mDATABASE_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbge-large-en\u001b[39m\u001b[38;5;124m'\u001b[39m, embedding_model\u001b[38;5;241m=\u001b[39mhf)\n\u001b[1;32m     18\u001b[0m chain \u001b[38;5;241m=\u001b[39m create_chain(vector_store)\n\u001b[0;32m---> 19\u001b[0m questions \u001b[38;5;241m=\u001b[39m get_questions(\u001b[43mcategory\u001b[49m, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mANNOTATION_DIR)\n\u001b[1;32m     20\u001b[0m answers \u001b[38;5;241m=\u001b[39m generate_answers(qa_chain\u001b[38;5;241m=\u001b[39mchain, questions\u001b[38;5;241m=\u001b[39mquestions)\n\u001b[1;32m     21\u001b[0m write_to_file \u001b[38;5;241m=\u001b[39m embedding_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_answers.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'category' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#clear cache on cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedding_name = 'bge-large-en'\n",
    "vector_store = load_vector_store_non_ollama_embedding(dir=DATABASE_PATH+'bge-large-en', embedding_model=hf)\n",
    "chain = create_chain(vector_store)\n",
    "questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "write_to_file = embedding_name + '_answers.txt'\n",
    "write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file for questions:  0  to  10  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  10  to  20  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  20  to  30  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  30  to  40  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  40  to  50  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  50  to  60  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  60  to  70  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  70  to  80  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  80  to  90  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  90  to  100  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  100  to  110  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  110  to  120  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  120  to  130  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  130  to  140  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  140  to  150  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  150  to  160  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  160  to  170  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  170  to  180  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  180  to  190  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  190  to  200  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  200  to  210  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  210  to  220  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  220  to  230  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  230  to  240  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  240  to  250  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  250  to  260  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  260  to  270  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  270  to  280  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  280  to  290  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  290  to  300  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  300  to  310  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  310  to  320  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  320  to  330  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  330  to  340  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  340  to  350  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  350  to  360  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  360  to  370  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  370  to  380  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  380  to  390  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  390  to  400  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  400  to  410  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  410  to  420  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  420  to  430  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  430  to  440  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  440  to  450  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  450  to  460  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  460  to  470  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  470  to  480  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  480  to  490  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  490  to  500  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  500  to  510  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  510  to  520  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  520  to  530  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  530  to  540  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  540  to  550  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  550  to  560  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  560  to  570  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  570  to  580  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  580  to  590  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  590  to  600  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  600  to  610  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  610  to  620  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  620  to  630  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  630  to  640  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  640  to  650  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  650  to  660  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  660  to  670  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  670  to  680  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  680  to  690  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  690  to  700  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  700  to  710  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  710  to  720  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  720  to  730  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  730  to  740  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  740  to  750  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  750  to  760  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n",
      "writing to file for questions:  760  to  770  to file:  /home/raj/nlp/cmu-rag/rveerara/system_outputs/system_output_1.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#clear cache on cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "TEST_QUESTION_PATH='/home/raj/nlp/cmu-rag/rveerara/system_outputs/'\n",
    "embedding_name = 'bge-large-en'\n",
    "vector_store = load_vector_store_non_ollama_embedding(dir=DATABASE_PATH+'bge-large-en', embedding_model=hf)\n",
    "chain = create_chain(vector_store)\n",
    "questions = get_questions(dir=TEST_QUESTION_PATH)\n",
    "print(questions)\n",
    "\n",
    "for i in range(0, len(questions), 10):\n",
    "    question_set = questions[i:min(i+10, len(questions))]\n",
    "    answers = generate_answers(qa_chain=chain, questions=question_set)\n",
    "    write_to_file = TEST_QUESTION_PATH + 'system_output_1.txt'\n",
    "    print(\"writing to file for questions: \", i, \" to \", i+10, \" to file: \", write_to_file)\n",
    "    write_answers(answers, write_to_file, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter, EmbeddingsFilter, FlashrankRerank\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException\n",
    "\n",
    "VECTOR_DATABASES_DIR_PATH = '/home/raj/nlp/cmu-rag/chroma_vector_database/'\n",
    "VECTOR_STORE_DEFAULT = 'bge-500-0.2'\n",
    "EMMBEDDING_DEFAULT = 'bge'\n",
    "ANNOTATION_DIR = '/home/raj/nlp/cmu-rag/rveerara/data/test/'\n",
    "QUESTIONS_FILE = ANNOTATION_DIR + 'questions.txt'\n",
    "ANSWERS_FILE = ANNOTATION_DIR + 'answers.txt'\n",
    "PROMPT_MESSAGE_LLAMA2 = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Use as few words as possible and keep the answer concise. Do not mention the context in your response.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "def load_vector_store(dir, embedding_model = OllamaEmbeddings()):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model)\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model, force=True)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def create_chain(vector_store, inference_model = Ollama(model='llama2'), prompt_message = PROMPT_MESSAGE_LLAMA2, embedding_model = OllamaEmbeddings()):\n",
    "    \n",
    "    rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "    rag_prompt_llama.messages[0].prompt.template = prompt_message\n",
    "    llm = inference_model\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
    "    # embeddings_filter = EmbeddingsFilter(embeddings=embedding_model, similarity_threshold=0.5)\n",
    "    compressor = FlashrankRerank()\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_llama\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def get_questions(file_name = QUESTIONS_FILE):\n",
    "    \n",
    "    if not file_name.endswith('questions.txt'):\n",
    "        raise ValueError(\"Invalid file name\")\n",
    "    \n",
    "    questions = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            questions.append(line.strip())\n",
    "    \n",
    "    return questions\n",
    "\n",
    "\n",
    "def generate_answers(qa_chain, questions):\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions to answer\")\n",
    "    if not qa_chain:\n",
    "        raise ValueError(\"No qa_chain to answer questions\")\n",
    "    \n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        if not question:\n",
    "            continue\n",
    "        answer = dict()\n",
    "        answer_raw = qa_chain.invoke(question)\n",
    "        answer[\"raw\"] = answer_raw\n",
    "        num_lines = answer_raw.count('\\n')\n",
    "        answer[\"num_lines\"] = num_lines\n",
    "        lines = answer_raw.split('\\n')\n",
    "        if num_lines == 0:\n",
    "            answer[\"processed\"] = lines[0]\n",
    "        else:\n",
    "            answer_lines = []\n",
    "            for line in lines:\n",
    "                if \"i don't know\" not in line.lower():\n",
    "                    answer_lines.append(line)\n",
    "            answer[\"processed\"] = \" \".join(answer_lines)\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def write_answers(answers, file_name, append = False):\n",
    "    try:\n",
    "        with open(file_name, 'a' if append else 'w') as f:\n",
    "            for answer in answers:\n",
    "                f.write(answer[\"processed\"] + '\\n')\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Error writing answers to file: \" + str(e))\n",
    "\n",
    "def do_rag_in_chunks(vector_store_path=VECTOR_DATABASES_DIR_PATH+VECTOR_STORE_DEFAULT,\n",
    "        embedding_model = None,\n",
    "        inference_model=Ollama(model='llama2'),\n",
    "        questions_file_name=QUESTIONS_FILE,\n",
    "        answers_file_name=ANSWERS_FILE,\n",
    "        append=False,\n",
    "        questions_to_process_at_once=50,):\n",
    "    if not embedding_model:\n",
    "        raise ValueError(\"Invalid embedding model\")\n",
    "    vector_store = load_vector_store(vector_store_path, embedding_model)\n",
    "    qa_chain = create_chain(vector_store, inference_model=inference_model, embedding_model=embedding_model)\n",
    "    questions = get_questions(file_name=questions_file_name)\n",
    "    num_questions = len(questions)\n",
    "    for i in range(0, num_questions, questions_to_process_at_once):\n",
    "        questions_chunk = questions[i:min(i+questions_to_process_at_once, num_questions)]\n",
    "        answers = generate_answers(qa_chain, questions_chunk)\n",
    "        write_answers(answers, answers_file_name, append=True) if i > 0 else write_answers(answers, answers_file_name, append=append)\n",
    "        print(f\"Processed {i+questions_to_process_at_once} questions out of {num_questions}\")\n",
    "\n",
    "\n",
    "def get_hugging_face_embedding_model():\n",
    "    model_name = \"BAAI/bge-large-en\"\n",
    "    model_kwargs = {\"device\": \"cuda\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    hf = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name, \n",
    "        model_kwargs=model_kwargs, \n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return hf\n",
    "\n",
    "def parse_arguments():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='RAG Chain')\n",
    "    parser.add_argument('--vector', type=str, default=VECTOR_DATABASES_DIR_PATH+VECTOR_STORE_DEFAULT, help='Path to the directory containing the vector store')\n",
    "    parser.add_argument('--embed', type=str, default=EMMBEDDING_DEFAULT, help='Embedding model to be used for loading embeddings')\n",
    "    parser.add_argument('--model', type=str, default='llama2', help='Model name to be used for read documents and generate answers')\n",
    "    parser.add_argument('--questions', type=str, default=QUESTIONS_FILE, help='Path to the file containing questions')\n",
    "    parser.add_argument('--answers', type=str, default=ANSWERS_FILE, help='Path to the file where answers will be written')\n",
    "    parser.add_argument('--append', type=bool, default=False, help='Append answers to the file')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "# embedder_path = '/home/raj/nlp/cmu-rag/chroma_vector_database/bge-500-0.2'\n",
    "# embedding_model = get_hugging_face_embedding_model()\n",
    "# vector_store = load_vector_store_non_ollama_embedding(embedder_path, embedding_model)\n",
    "# chain = create_chain(vector_store, embedding_model=embedding_model)\n",
    "# questions = get_questions(QUESTIONS_FILE)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     print(\"Starting RAG Chain\")\n",
    "\n",
    "#     args = parse_arguments()\n",
    "#     vector_store_path = args.vector\n",
    "#     embedding_model_option = args.embed\n",
    "#     model_name = args.model\n",
    "#     questions_file_name = args.questions\n",
    "#     answers_file_name = args.answers\n",
    "#     append = args.append\n",
    "\n",
    "#     embedding_model = get_hugging_face_embedding_model() if embedding_model_option == 'bge' else OllamaEmbeddings(model=embedding_model_option)\n",
    "#     inference_model = Ollama(model=model_name)\n",
    "    \n",
    "#     print(\"Starting RAG Chain with the following parameters:\")\n",
    "#     print(f\"\\tVector Store Path: {vector_store_path}\")\n",
    "#     print(f\"\\tEmbedding Model: {embedding_model.__class__.__name__}\")\n",
    "#     print((f\"\\tInference Model: {inference_model.__class__.__name__}\") + (f\" ({model_name})\" if model_name else \"\"))\n",
    "#     print(f\"\\tQuestions File: {questions_file_name}\")\n",
    "#     print(f\"\\tAnswers File: {answers_file_name}\")\n",
    "#     print(f\"\\tAppend: {append}\")\n",
    "\n",
    "#     do_rag_in_chunks(vector_store_path=vector_store_path,\n",
    "#         embedding_model=embedding_model,\n",
    "#         inference_model=inference_model,\n",
    "#         questions_file_name=questions_file_name,\n",
    "#         answers_file_name=answers_file_name,\n",
    "#         append=append)\n",
    "    \n",
    "#     print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_path = '/home/raj/nlp/cmu-rag/chroma_vector_database/bge-3000-0.2'\n",
    "embedding_model = get_hugging_face_embedding_model()\n",
    "vector_store = load_vector_store(embedder_path, embedding_model)\n",
    "# chain = create_chain(vector_store, embedding_model=embedding_model)\n",
    "questions = get_questions(QUESTIONS_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which conference was the paper titled \"Why do Nearest Neighbor Language Models Work?\" published ?\n",
      "Document 1:\n",
      "\n",
      "Title: Why do Nearest Neighbor Language Models Work?\n",
      "Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.\n",
      "Authors: Frank F. Xu, Uri Alon, Graham Neubig\n",
      "Publication Venue: International Conference on Machine Learning, ICML, Int Conf Mach Learn\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Title: Large Language Models Enable Few-Shot Clustering\n",
      "Abstract: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.\n",
      "Authors: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig\n",
      "Publication Venue: arXiv.org, ArXiv\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "\n",
      "Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach\n",
      "Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.\n",
      "Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki\n",
      "Publication Venue: arXiv.org, ArXiv\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction\n",
      "Abstract: In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.\n",
      "Authors: Zhisong Zhang, Emma Strubell, E. Hovy\n",
      "Publication Venue: Not Available\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M. Towhidul Islam Tonmoy, Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das, Paris, A. Sridhar, Erik Visser, Improved, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, Roformer, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Stanford, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Susan Zhang, Stephen Roller, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona T. Diab, Xi Xian Li, Todor Victoria Lin, Myle Ott, Kurt Shuster, Punit Daniel Simig, Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022, Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul F. Chris-tiano\n",
      "Publication Venue: Conference on Empirical Methods in Natural Language Processing, Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data\n",
      "Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.\n",
      "Authors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu\n",
      "Publication Venue: Annual Meeting of the Association for Computational Linguistics, Annu Meet Assoc Comput Linguistics, Meeting of the Association for Computational Linguistics, ACL, Meet Assoc Comput Linguistics\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "Title: KALE: Using a K-Sparse Projector for Lexical Expansion\n",
      "Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.\n",
      "Authors: Luís Borges, Bruno Martins, Jamie Callan\n",
      "Publication Venue: International Conference on the Theory of Information Retrieval, Int Conf Theory Inf Retr, ICTIR\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations\n",
      "Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.\n",
      "Authors: Victoria Lin, Louis-Philippe Morency\n",
      "Publication Venue: Annual Meeting of the Association for Computational Linguistics, Annu Meet Assoc Comput Linguistics, Meeting of the Association for Computational Linguistics, ACL, Meet Assoc Comput Linguistics\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\n",
      "Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.\n",
      "Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov\n",
      "Publication Venue: Conference on Empirical Methods in Natural Language Processing, Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions\n",
      "Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.\n",
      "Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency\n",
      "Publication Venue: Annual Meeting of the Association for Computational Linguistics, Annu Meet Assoc Comput Linguistics, Meeting of the Association for Computational Linguistics, ACL, Meet Assoc Comput Linguistics\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "Title: It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk\n",
      "Abstract: Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.\n",
      "Authors: Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley\n",
      "Publication Venue: Not Available\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "\n",
      "Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\n",
      "Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .\n",
      "Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley\n",
      "Publication Venue: arXiv.org, ArXiv\n",
      "Year of Publication: 2023\n",
      "Summary: Not Available\n",
      "2890\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 15})\n",
    "# embeddings_filter = EmbeddingsFilter(embeddings=embedding_model, similarity_threshold=0.5)\n",
    "\n",
    "embeddings = get_hugging_face_embedding_model()\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.6)\n",
    "\n",
    "flashranker = FlashrankRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "# question = questions[0]\n",
    "question = \"In which conference was the paper titled \\\"Why do Nearest Neighbor Language Models Work?\\\" published ?\"\n",
    "print(question)\n",
    "\n",
    "docs= compression_retriever.get_relevant_documents(question, k=5)\n",
    "# doc1 = compression_retriever.get_relevant_documents(question, k=50)\n",
    "\n",
    "# print(len(doc1))\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for doc in docs:\n",
    "    cnt += len(word_tokenize(doc.page_content))\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "str1 = \"\"\"\n",
    "\n",
    "Instructor  Saadati teaches course number  49750 ,titled ' Integrated Thinking for Innovation' in semester  Fall 2023, under the category/department of  Integrated Innovation Institute, for section  A1. The course consists of  6.0 units and is taught on the following days:  in the building '  ', which is located at   . The course begins at    and ends at   .\n",
    "Instructor  Carja teaches course number  02702 ,titled ' Computational Biology Seminar' in semester  Spring 2024, under the category/department of  Computational Biology, for section  A. The course consists of  3.0 units and is taught on the following days: Friday in the building ' TBA', which is located at  Pittsburgh, Pennsylvania. The course begins at  1030AM and ends at  1150AM.\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "str2 = \"\"\"\n",
    "Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting\n",
    "Abstract: Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.\n",
    "Authors: Emmy Liu, Aditi Chaudhary, Graham Neubig\n",
    "Publication Venue: Conference on Empirical Methods in Natural Language Processing, Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP\n",
    "Year of Publication: 2023\n",
    "Summary: Not Available\n",
    "\"\"\"\n",
    "print(len(str2))\n",
    "print(len(word_tokenize(str2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
