{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r ../requirements.txt\n",
    "# %pip install --quiet --upgrade  langchain langchain-community langchainhub gpt4all chromadb bs4 torch transformers\n",
    "# !pip freeze >> ../requirements.txt\n",
    "\n",
    "import os, time\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings:  ['tinyllama', 'llama2', 'gemma', 'everythiglm', 'mistral', 'neural-chat', 'openchat']\n",
      "Processing text file:  history_of_cmu\n",
      "Processing text file:  Kiltie Band\n",
      "Processing text file:  spring carnival and renuion weekend.txt\n",
      "Processing text file:  Tartan Facts\n",
      "Processing text file:  academic_calendars\n",
      "Processing text file:  program_handbooks\n",
      "Processing text file:  lti_faculty\n",
      "Processing text file:  history_of_scs\n",
      "Processing text file:  About Scottie\n",
      "Processing text file:  lti_papers_metadata\n",
      "Processing text file:  Buggy News\n",
      "Processing text file:  lti_programs\n",
      "Splitting documents into chunks\n",
      "Creating embeddings...\n",
      "Preparing embedding:  tinyllama\n",
      "Finished embedding: tinyllama in 151.55805349349976 seconds\n",
      "Preparing embedding:  llama2\n",
      "Finished embedding: llama2 in 703.6288022994995 seconds\n",
      "Preparing embedding:  gemma\n",
      "Finished embedding: gemma in 816.3149952888489 seconds\n",
      "Preparing embedding:  everythiglm\n",
      "Error:  Error raised by inference API HTTP code: 404, {\"error\":\"model 'everythiglm' not found, try pulling it first\"}\n",
      "Preparing embedding:  mistral\n",
      "Finished embedding: mistral in 755.257940530777 seconds\n",
      "Preparing embedding:  neural-chat\n",
      "Finished embedding: neural-chat in 755.5379438400269 seconds\n",
      "Preparing embedding:  openchat\n",
      "Finished embedding: openchat in 754.9983727931976 seconds\n",
      "Done all embeddings of all documents in 3937.2961082458496 seconds!\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/home/raj/nlp/cmu-rag/rag'\n",
    "BASE_DIR_TXT_FILES = '/home/raj/nlp/cmu-rag/helper/combined_txt_files/'\n",
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "\n",
    "CPU_RAM = 26\n",
    "\n",
    "# os.chdir(BASE_DIR)\n",
    "# !touch embedding_options.txt\n",
    "# !ollama list > embedding_options.txt\n",
    "\n",
    "embedding_options = ['tinyllama', 'llama2', 'gemma', 'everythiglm', 'mistral', 'neural-chat', 'openchat']\n",
    "# try:\n",
    "#     with open('embedding_options.txt', 'r') as file:\n",
    "#         for line in file.readlines()[1:]:\n",
    "#             line_content = line.split()\n",
    "#             if float(line_content[2]) < 0.8 * CPU_RAM or line_content[3] == 'MB':\n",
    "#                 embedding_options.append(line_content[0].split(\":\")[0])\n",
    "# except Exception as e:\n",
    "#     print(\"Error: \", e)\n",
    "# finally:\n",
    "#     file.close()\n",
    "#     !rm embedding_options.txt\n",
    "\n",
    "print(\"Available embeddings: \", embedding_options)\n",
    "\n",
    "documents = []\n",
    "for file in os.listdir(BASE_DIR_TXT_FILES):\n",
    "    if \"schedule\" not in file:\n",
    "        print(\"Processing text file: \", file)\n",
    "        loader = TextLoader(BASE_DIR_TXT_FILES + file)\n",
    "        documents.extend(loader.load())\n",
    "        \n",
    "\n",
    "print(\"Splitting documents into chunks\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_text = splitter.split_documents(documents)\n",
    "\n",
    "# print(doc_text)\n",
    "\n",
    "embedding_times = [0] * len(embedding_options)\n",
    "\n",
    "print(\"Creating embeddings...\")\n",
    "for embedding in embedding_options:\n",
    "    print(\"Preparing embedding: \", embedding)\n",
    "    start_time = time.time()\n",
    "    vector_database = None\n",
    "    try: \n",
    "        # chroma_client = chromadb.PersistentClient(path=DATABASE_PATH)\n",
    "        # collection = chroma_client.create_collection(name=embedding, embedding_function=OllamaEmbeddings(model = embedding))\n",
    "        # collection.add_documents(all_text)\n",
    "        vector_database = Chroma.from_documents(documents = doc_text, embedding=OllamaEmbeddings(model = embedding), persist_directory=DATABASE_PATH+embedding)\n",
    "        vector_database.persist()\n",
    "    except InvalidDimensionException as e:\n",
    "        print(\"Invalid dimension for embedding: \", embedding)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        continue\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "    print(\"Finished embedding: \" + embedding + \" in \" + str(end_time - start_time) + \" seconds\")\n",
    "    embedding_times[embedding_options.index(embedding)] = end_time - start_time\n",
    "print(\"Done all embeddings of all documents in \" + str(sum(embedding_times)) + \" seconds!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
