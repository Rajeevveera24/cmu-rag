{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/raj/nlp/cmu-rag/rag'\n",
    "BASE_DIR_TXT_FILES = '/home/raj/nlp/cmu-rag/helper/combined_txt_files/'\n",
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "\n",
    "CPU_RAM = 26\n",
    "\n",
    "embedding_options = ['tinyllama', 'llama2', 'gemma', 'everythiglm', 'mistral', 'neural-chat', 'openchat']\n",
    "\n",
    "print(\"Available embeddings: \", embedding_options)\n",
    "\n",
    "documents = []\n",
    "for file in os.listdir(BASE_DIR_TXT_FILES):\n",
    "    if \"schedule\" not in file:\n",
    "        print(\"Processing text file: \", file)\n",
    "        loader = TextLoader(BASE_DIR_TXT_FILES + file)\n",
    "        documents.extend(loader.load())\n",
    "        \n",
    "\n",
    "print(\"Splitting documents into chunks\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_text = splitter.split_documents(documents)\n",
    "\n",
    "# print(doc_text)\n",
    "\n",
    "embedding_times = [0] * len(embedding_options)\n",
    "\n",
    "print(\"Creating embeddings...\")\n",
    "for embedding in embedding_options:\n",
    "    print(\"Preparing embedding: \", embedding)\n",
    "    start_time = time.time()\n",
    "    vector_database = None\n",
    "    try: \n",
    "        vector_database = Chroma.from_documents(documents = doc_text, embedding=OllamaEmbeddings(model = embedding), persist_directory=DATABASE_PATH+embedding)\n",
    "        vector_database.persist()\n",
    "    except InvalidDimensionException as e:\n",
    "        print(\"Invalid dimension for embedding: \", embedding)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        continue\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "    print(\"Finished embedding: \" + embedding + \" in \" + str(end_time - start_time) + \" seconds\")\n",
    "    embedding_times[embedding_options.index(embedding)] = end_time - start_time\n",
    "print(\"Done all embeddings of all documents in \" + str(sum(embedding_times)) + \" seconds!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
