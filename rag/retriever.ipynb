{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "# embedding_name = 'llama2'\n",
    "# persist_directory = DATABASE_PATH + embedding_name\n",
    "# embedding = OllamaEmbeddings(model=embedding_name)\n",
    "# vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "EMBEDDING_OPTIONS = ['tinyllama', 'llama2', 'gemma', 'mistral', 'neural-chat', 'openchat']\n",
    "VECTOR_STORE_DIRECTORIES = [DATABASE_PATH + embedding_name for embedding_name in EMBEDDING_OPTIONS]\n",
    "\n",
    "QUESTION_CATEGORIES = ['history']\n",
    "ANNOTATION_DIR = '/home/raj/nlp/cmu-rag/annotation/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(dir, embedding_name):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name))\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name), force=True)\n",
    "    return vector_store\n",
    "\n",
    "def create_chain(vector_store, llm_model = 'llama2'):\n",
    "    rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "    prompt_message = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use as few words as possible and keep the answer concise. Do not mention the context in your response.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "    rag_prompt_llama.messages[0].prompt.template = prompt_message\n",
    "\n",
    "    llm = Ollama(model = llm_model)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_llama\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(category, dir = ANNOTATION_DIR):\n",
    "    questions = []\n",
    "    for file in os.listdir(dir + category):\n",
    "        if file.endswith('questions.txt'):\n",
    "            with open(dir + category + '/' + file, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    questions.append(line.strip())\n",
    "    return questions\n",
    "\n",
    "def generate_answers(qa_chain, questions):\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions to answer\")\n",
    "    if not qa_chain:\n",
    "        raise ValueError(\"No qa_chain to answer questions\")\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        answer = dict()\n",
    "        answer_raw = qa_chain.invoke(question)\n",
    "        answer[\"raw\"] = answer_raw\n",
    "        num_lines = answer_raw.count('\\n')\n",
    "        answer[\"num_lines\"] = num_lines\n",
    "        lines = answer_raw.split('\\n')\n",
    "        if num_lines == 0:\n",
    "            answer[\"processed\"] = lines[0] if \"i don't know\" not in lines[0].lower() else \"I do not know\"\n",
    "        else:\n",
    "            answer_lines = []\n",
    "            for line in lines:\n",
    "                if \"i don't know\" not in line.lower():\n",
    "                    answer_lines.append(line)\n",
    "            answer[\"processed\"] = \" \".join(answer_lines)\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def write_answers(answers, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for answer in answers:\n",
    "            f.write(answer[\"processed\"] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering questions for category: history with embedding tinyllama\n"
     ]
    }
   ],
   "source": [
    "for dir in VECTOR_STORE_DIRECTORIES:\n",
    "    embedding_name = EMBEDDING_OPTIONS[VECTOR_STORE_DIRECTORIES.index(dir)]\n",
    "    vector_store = load_vector_store(dir, embedding_name)\n",
    "    chain = create_chain(vector_store, embedding_name)\n",
    "    for category in QUESTION_CATEGORIES:\n",
    "        print(\"Answering questions for category: {} with embedding {}\".format(category, embedding_name))\n",
    "        questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "        answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "        write_to_file = embedding_name + '_answers.txt'\n",
    "        write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n",
    "        print(f\"Answers written to {ANNOTATION_DIR + category + write_to_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
