{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from chromadb.errors import InvalidDimensionException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = '/home/raj/nlp/cmu-rag/rag/chroma/txt/'\n",
    "# embedding_name = 'llama2'\n",
    "# persist_directory = DATABASE_PATH + embedding_name\n",
    "# embedding = OllamaEmbeddings(model=embedding_name)\n",
    "# vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "EMBEDDING_OPTIONS = ['tinyllama', 'llama2', 'gemma', 'mistral', 'neural-chat', 'openchat']\n",
    "VECTOR_STORE_DIRECTORIES = [DATABASE_PATH + embedding_name for embedding_name in EMBEDDING_OPTIONS]\n",
    "\n",
    "QUESTION_CATEGORIES = ['history']\n",
    "ANNOTATION_DIR = '/home/raj/nlp/cmu-rag/annotation/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(dir, embedding_name):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name))\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=OllamaEmbeddings(model=embedding_name), force=True)\n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store_non_ollama_embedding(dir, embedding_model):\n",
    "    try:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model)\n",
    "    except InvalidDimensionException:\n",
    "        vector_store = Chroma(persist_directory=dir, embedding_function=embedding_model, force=True)\n",
    "    return vector_store\n",
    "\n",
    "def create_chain(vector_store, llm_model = 'llama2'):\n",
    "    rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "    prompt_message = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use as few words as possible and keep the answer concise. Do not mention the context in your response.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"\n",
    "\n",
    "    rag_prompt_llama.messages[0].prompt.template = prompt_message\n",
    "\n",
    "    llm = Ollama(model = llm_model)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_llama\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(category, dir = ANNOTATION_DIR):\n",
    "    questions = []\n",
    "    for file in os.listdir(dir + category):\n",
    "        if file.endswith('questions.txt'):\n",
    "            with open(dir + category + '/' + file, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    questions.append(line.strip())\n",
    "    return questions\n",
    "\n",
    "def generate_answers(qa_chain, questions):\n",
    "    if not questions:\n",
    "        raise ValueError(\"No questions to answer\")\n",
    "    if not qa_chain:\n",
    "        raise ValueError(\"No qa_chain to answer questions\")\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        if not question:\n",
    "            continue\n",
    "        answer = dict()\n",
    "        answer_raw = qa_chain.invoke(question)\n",
    "        answer[\"raw\"] = answer_raw\n",
    "        num_lines = answer_raw.count('\\n')\n",
    "        answer[\"num_lines\"] = num_lines\n",
    "        lines = answer_raw.split('\\n')\n",
    "        if num_lines == 0:\n",
    "            answer[\"processed\"] = lines[0] if \"i don't know\" not in lines[0].lower() else \"I do not know\"\n",
    "        else:\n",
    "            answer_lines = []\n",
    "            for line in lines:\n",
    "                if \"i don't know\" not in line.lower():\n",
    "                    answer_lines.append(line)\n",
    "            answer[\"processed\"] = \" \".join(answer_lines)\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "def write_answers(answers, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for answer in answers:\n",
    "            f.write(answer[\"processed\"] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering questions for category: history with embedding tinyllama\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswering questions for category: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with embedding \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(category, embedding_name))\n\u001b[1;32m      7\u001b[0m questions \u001b[38;5;241m=\u001b[39m get_questions(category, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mANNOTATION_DIR)\n\u001b[0;32m----> 8\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m write_to_file \u001b[38;5;241m=\u001b[39m embedding_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_answers.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m write_answers(answers, ANNOTATION_DIR \u001b[38;5;241m+\u001b[39m category \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m write_to_file)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mgenerate_answers\u001b[0;34m(qa_chain, questions)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 20\u001b[0m answer_raw \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m answer_raw\n\u001b[1;32m     22\u001b[0m num_lines \u001b[38;5;241m=\u001b[39m answer_raw\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cmu_rag/lib/python3.12/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/cmu_rag/lib/python3.12/site-packages/langchain_core/runnables/base.py:2712\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   2700\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2701\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   2702\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2710\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2711\u001b[0m         ]\n\u001b[0;32m-> 2712\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   2713\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2714\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/cmu_rag/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/cmu_rag/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dir in VECTOR_STORE_DIRECTORIES:\n",
    "    embedding_name = EMBEDDING_OPTIONS[VECTOR_STORE_DIRECTORIES.index(dir)]\n",
    "    vector_store = load_vector_store(dir, embedding_name)\n",
    "    chain = create_chain(vector_store, embedding_name)\n",
    "    for category in QUESTION_CATEGORIES:\n",
    "        print(\"Answering questions for category: {} with embedding {}\".format(category, embedding_name))\n",
    "        questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "        answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "        write_to_file = embedding_name + '_answers.txt'\n",
    "        write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n",
    "        print(f\"Answers written to {ANNOTATION_DIR + category + '/' + write_to_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#clear cache on cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedding_name = 'bge-large-en'\n",
    "vector_store = load_vector_store_non_ollama_embedding(dir=DATABASE_PATH+'bge-large-en', embedding_model=hf)\n",
    "chain = create_chain(vector_store)\n",
    "questions = get_questions(category, dir=ANNOTATION_DIR)\n",
    "answers = generate_answers(qa_chain=chain, questions=questions)\n",
    "write_to_file = embedding_name + '_answers.txt'\n",
    "write_answers(answers, ANNOTATION_DIR + category + '/' + write_to_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
